# Empowering Audio LLM with End-to-End Speech Generation via Streaming Text-to-Unit Conversion

The research goal is to develop an efficient paradigm to enable the LLM with the ability of streaming audio generation with low latency and high quality. The current pipeline consists three steps:

- Step 1: Data Preparation. We build the text-unit pair for later training. In each pair, there are text tokens generated by the tokenizer of the given LLM, and the unit tokens are generated by an unit vocoder that can transform between discrete unit tokens and audio wave at ease. Then, we use forced alignment to map each unit token with its corresponding text token.
- Step 2: Single Token Training. In this step, we leverage the aligned text-unit pair to train the model with single text token data. It helps the model learn the generation of each token separately from scratch.
- Step 3: Sentence Level Training. In this step, we train with data of one or more sentences to improve the model's ability in context.

Here is the brief introduction of this codebase. Note that there are absolute paths in the code that might need adjustment.

## Data Preparation

Under `Alignment` folder, there are codes for the forced alignment (unsupervised training) process. The model is defined in `forced_aligner.py`, and `train.sh` specifies the usage of the training process. After the training, use `build_t2u_pair_parallel_align.py` to inference on the same dataset and build text-unit pair with alignment.

To adapt Qwen2-Audio instead of DiVA, use `train_qwen2audio.sh` for training and `build_t2u_pair_parallel_align_qwen2audio.py` for inference.

We also tried aligning the tokens with character level information. Under `Alignment-CTC` folder, `build_t2u_pair_parallel_align.py` is used to build such alignment. And `build_merged_t2u_pair.py` is to combine the two alignment strategy.

## Training

Code for Step 2 & 3 is under `TTU` folder. There are a few scripts for different model design, and the final version is defined in `ttu_llama_canine_inference.py`. To train the model, adjust the config parameters defined at the beginning of `ttu_llama_canine_inference`, then run `bash train_ttu_llama_canine.sh $NGPU`. An example of the config parameters are as follows:

```python
# TrainingConfig
LR = 1e-5
BATCH_SIZE = 8
EPOCHS = 4
WANDB = False

DATASET_NAME = "commonvoice"
assert DATASET_NAME in ["mls", "commonvoice"]

# Data Config
CLEAN_TUPAIR = True
ONLY_FIRST = False
SEPERATED = False
assert not (ONLY_FIRST and SEPERATED)

DOUBLE = False
NO_LAST = False

# ModelConfig
EMBED_DIM = 1280  # Embedding dimension
FFN_DIM = 2048 # Default 2048
DROPOUT = 0.1 # Default 0.1
NUM_HEADS = 8 
NUM_LAYERS = 6
SRC_MASK_TYPE = "causal" # none or causal
TGT_MASK_TYPE = "independent" #"independent" or causal or lookbehind-x
MEMORY_MASK_TYPE = "independent" # independent or causal or lookbehind-x

USE_CANINE = True
CANINE_REDUCE = "sum"

ENCODER_TYPE = "llama3"
assert ENCODER_TYPE in ["llama3", "llama3_embed", "qwen2audio"]

CONTINUE_TRAINING_PATH = None
```

Note that `SEPERATED` config controls the data format. Set it to `True` in Step 2 only.

## Demo

The `demo` folder provides a gradio app for the current project. Use `run_app.sh` to run. The checkpoint path is hardcoded in `model.py`. Note that the current inference code is for `independent` attention masks only.